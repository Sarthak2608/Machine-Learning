{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\welcome\\Anaconda3\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"He jump jumping jumped I am Sarthak Jain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting sentence\n",
    "w=word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'jump', 'jumping', 'jumped', 'I', 'am', 'Sarthak', 'Jain']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'jump', 'jumping', 'jumped', 'I', 'am', 'Sarthak', 'Jain']\n"
     ]
    }
   ],
   "source": [
    "#splitting sentence using reges\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer('[a-zA-Z]+')\n",
    "s_r=tokenizer.tokenize(sentence)\n",
    "print(s_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#stopward removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw=set(stopwords.words(\"english\"))\n",
    "def remove_stopword(sentence_,sw):\n",
    "    return [i for i in sentence_ if i not in sw]\n",
    "\n",
    "w=remove_stopword(sentence.split(),sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'jump', 'jumping', 'jumped', 'I', 'Sarthak', 'Jain']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stem=SnowballStemmer('english')\n",
    "snow_stem.stem('Jumping')\n",
    "def Change_word_into_baseform(w):\n",
    "    w=[snow_stem.stem(i) for i in w]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'jump', 'jump', 'jump', 'i', 'sarthak', 'jain']\n"
     ]
    }
   ],
   "source": [
    "w=Change_word_into_baseform(w)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[\n",
    "    \"I like movies\",\n",
    "    \"I hate movies\",\n",
    "    \"I hate movies\",\n",
    "    \"I don't like movies\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer('[a-zA-Z]+')\n",
    "\n",
    "#stopward removal\n",
    "from nltk.corpus import stopwords\n",
    "sw=set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopword(sentence_,sw):\n",
    "    return [i for i in sentence_ if i not in sw]\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stem=SnowballStemmer('english')\n",
    "def Change_word_into_baseform(sentence_):\n",
    "    return [snow_stem.stem(i) for i in sentence_]\n",
    "    \n",
    "\n",
    "\n",
    "def myTokenizer(sentence):\n",
    "    sentence_list=tokenizer.tokenize(sentence)\n",
    "    sentence_list=Change_word_into_baseform(sentence_list)\n",
    "    sentence_list=remove_stopword(sentence_list,sw)\n",
    "    print(sentence_list)\n",
    "    return sentence_list\n",
    "\n",
    "#if the sentence is [w1 w2 w3 w4 ] then key will be [wa] ,[wa wa+1] , [wa..wb] ,,...\n",
    "a=1  \n",
    "b=3\n",
    "cv=CountVectorizer(tokenizer=myTokenizer,ngram_range=(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'movi']\n",
      "['hate', 'movi']\n",
      "['hate', 'movi']\n",
      "['like', 'movi']\n"
     ]
    }
   ],
   "source": [
    "data_vector=cv.fit_transform(data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1]\n",
      " [1 1 0 0 1]\n",
      " [1 1 0 0 1]\n",
      " [0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(data_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(data_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'like': 2, 'movi': 4, 'like movi': 3, 'hate': 0, 'hate movi': 1}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would', 'like', 'sarthak', 'express', 'deepest']\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "\n",
    "test_data=[\n",
    "    'I would like Sarthak to express my deepest'\n",
    "]\n",
    "test_vector=cv.transform(test_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf normalisation\n",
    "#N=total distinct labels\n",
    "#value of word1=frequency of the word in the current example *  log(N/(1+total frequency in all example))\n",
    "#if the word is occuring in almost all document then the weight of the word is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer('[a-zA-Z]+')\n",
    "\n",
    "#stopward removal\n",
    "from nltk.corpus import stopwords\n",
    "sw=set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopword(sentence_,sw):\n",
    "    return [i for i in sentence_ if i not in sw or i in [\"not\"]]\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stem=SnowballStemmer('english')\n",
    "def Change_word_into_baseform(sentence_):\n",
    "    return [snow_stem.stem(i) for i in sentence_]\n",
    "    \n",
    "\n",
    "\n",
    "def myTokenizer(sentence):\n",
    "    sentence_list=tokenizer.tokenize(sentence)\n",
    "    sentence_list=Change_word_into_baseform(sentence_list)\n",
    "    sentence_list=remove_stopword(sentence_list,sw)\n",
    "    print(sentence_list)\n",
    "    return sentence_list\n",
    "\n",
    "#if the sentence is [w1 w2 w3 w4 ] then key will be [wa] ,[wa wa+1] , [wa..wb] ,,...\n",
    "a=1  \n",
    "b=3\n",
    "tv=TfidfVectorizer(tokenizer=myTokenizer,ngram_range=(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'movi']\n",
      "['hate', 'movi']\n",
      "['hate', 'movi']\n",
      "['not', 'like', 'movi']\n"
     ]
    }
   ],
   "source": [
    "data=[\n",
    "    \"I like movies\",\n",
    "    \"I hate movies\",\n",
    "    \"I hate movies\",\n",
    "    \"I do not like movies\"\n",
    "]\n",
    "data_tv_vector=tv.fit_transform(data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.64043405, 0.64043405, 0.42389674,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.64043405, 0.64043405, 0.        , 0.        , 0.42389674,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.64043405, 0.64043405, 0.        , 0.        , 0.42389674,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.37102215, 0.37102215, 0.24557576,\n",
       "        0.47059455, 0.47059455, 0.47059455]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tv_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'like': 2,\n",
       " 'movi': 4,\n",
       " 'like movi': 3,\n",
       " 'hate': 0,\n",
       " 'hate movi': 1,\n",
       " 'not': 5,\n",
       " 'not like': 6,\n",
       " 'not like movi': 7}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
